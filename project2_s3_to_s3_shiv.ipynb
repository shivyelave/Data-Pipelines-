{
	"cells": [
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"\"\"\"\n",
				"@Author: Shivraj Yelave\n",
				"@Date: 2024-10-28\n",
				"@Last Modified by: Shivraj Yelave\n",
				"@Last Modified time: 2024-10-28\n",
				"@Title: Glue Pipeline using Glue notebook\n",
				"\"\"\""
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"source": [
				"# AWS Glue Studio Notebook\n",
				"##### You are now running a AWS Glue Studio notebook; To start using your notebook you need to start an AWS Glue Interactive Session.\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"source": [
				"#### Optional: Run this cell to see available notebook commands (\"magics\").\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"editable": true,
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"%help"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"source": [
				"####  Run this cell to set up and start your interactive session.\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 1,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"Welcome to the Glue Interactive Sessions Kernel\n",
						"For more information on available magic commands, please type %help in any new cell.\n",
						"\n",
						"Please view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\n",
						"Installed kernel version: 1.0.5 \n",
						"Current idle_timeout is None minutes.\n",
						"idle_timeout has been set to 2880 minutes.\n",
						"Setting Glue version to: 4.0\n",
						"Previous worker type: None\n",
						"Setting new worker type to: G.1X\n",
						"Previous number of workers: None\n",
						"Setting new number of workers to: 5\n",
						"Trying to create a Glue session for the kernel.\n",
						"Session Type: glueetl\n",
						"Worker Type: G.1X\n",
						"Number of Workers: 5\n",
						"Idle Timeout: 2880\n",
						"Session ID: 60e5f037-59e0-40d3-9f1d-b1994c57b2f6\n",
						"Applying the following default arguments:\n",
						"--glue_kernel_version 1.0.5\n",
						"--enable-glue-datacatalog true\n",
						"Waiting for session 60e5f037-59e0-40d3-9f1d-b1994c57b2f6 to get into ready status...\n",
						"Session 60e5f037-59e0-40d3-9f1d-b1994c57b2f6 has been created.\n",
						"GlueArgumentError: the following arguments are required: --JOB_NAME\n"
					]
				}
			],
			"source": [
				"%idle_timeout 2880\n",
				"%glue_version 4.0\n",
				"%worker_type G.1X\n",
				"%number_of_workers 5\n",
				"\n",
				"import sys\n",
				"from awsglue.transforms import *\n",
				"from awsglue.utils import getResolvedOptions\n",
				"from awsglue.context import GlueContext\n",
				"from awsglue.job import Job\n",
				"from pyspark.sql import SparkSession\n",
				"from pyspark.sql import functions as F\n",
				"\n",
				"# Define the application name\n",
				"app_name = \"MyGlueJob\"\n",
				"\n",
				"# Initialize the SparkSession with the application name\n",
				"spark = SparkSession.builder \\\n",
				"    .appName(app_name) \\\n",
				"    .getOrCreate()\n",
				"\n",
				"\n",
				"# Create Glue context\n",
				"glueContext = GlueContext(spark.sparkContext)\n",
				"\n",
				"\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 5,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"root\n",
						"|-- instant: long\n",
						"|-- dteday: string\n",
						"|-- season: long\n",
						"|-- yr: long\n",
						"|-- mnth: long\n",
						"|-- hr: long\n",
						"|-- holiday: long\n",
						"|-- weekday: long\n",
						"|-- workingday: long\n",
						"|-- weathersit: long\n",
						"|-- temp: double\n",
						"|-- atemp: double\n",
						"|-- hum: double\n",
						"|-- windspeed: double\n",
						"|-- casual: long\n",
						"|-- registered: long\n",
						"|-- cnt: long\n",
						"\n",
						"+-------+----------+------+---+----+---+-------+-------+----------+----------+----+------+----+---------+------+----------+---+\n",
						"|instant|    dteday|season| yr|mnth| hr|holiday|weekday|workingday|weathersit|temp| atemp| hum|windspeed|casual|registered|cnt|\n",
						"+-------+----------+------+---+----+---+-------+-------+----------+----------+----+------+----+---------+------+----------+---+\n",
						"|      1|2011-01-01|     1|  0|   1|  0|      0|      6|         0|         1|0.24|0.2879|0.81|      0.0|     3|        13| 16|\n",
						"|      2|2011-01-01|     1|  0|   1|  1|      0|      6|         0|         1|0.22|0.2727| 0.8|      0.0|     8|        32| 40|\n",
						"|      3|2011-01-01|     1|  0|   1|  2|      0|      6|         0|         1|0.22|0.2727| 0.8|      0.0|     5|        27| 32|\n",
						"|      4|2011-01-01|     1|  0|   1|  3|      0|      6|         0|         1|0.24|0.2879|0.75|      0.0|     3|        10| 13|\n",
						"|      5|2011-01-01|     1|  0|   1|  4|      0|      6|         0|         1|0.24|0.2879|0.75|      0.0|     0|         1|  1|\n",
						"|      6|2011-01-01|     1|  0|   1|  5|      0|      6|         0|         2|0.24|0.2576|0.75|   0.0896|     0|         1|  1|\n",
						"|      7|2011-01-01|     1|  0|   1|  6|      0|      6|         0|         1|0.22|0.2727| 0.8|      0.0|     2|         0|  2|\n",
						"|      8|2011-01-01|     1|  0|   1|  7|      0|      6|         0|         1| 0.2|0.2576|0.86|      0.0|     1|         2|  3|\n",
						"|      9|2011-01-01|     1|  0|   1|  8|      0|      6|         0|         1|0.24|0.2879|0.75|      0.0|     1|         7|  8|\n",
						"|     10|2011-01-01|     1|  0|   1|  9|      0|      6|         0|         1|0.32|0.3485|0.76|      0.0|     8|         6| 14|\n",
						"|     11|2011-01-01|     1|  0|   1| 10|      0|      6|         0|         1|0.38|0.3939|0.76|   0.2537|    12|        24| 36|\n",
						"|     12|2011-01-01|     1|  0|   1| 11|      0|      6|         0|         1|0.36|0.3333|0.81|   0.2836|    26|        30| 56|\n",
						"|     13|2011-01-01|     1|  0|   1| 12|      0|      6|         0|         1|0.42|0.4242|0.77|   0.2836|    29|        55| 84|\n",
						"|     14|2011-01-01|     1|  0|   1| 13|      0|      6|         0|         2|0.46|0.4545|0.72|   0.2985|    47|        47| 94|\n",
						"|     15|2011-01-01|     1|  0|   1| 14|      0|      6|         0|         2|0.46|0.4545|0.72|   0.2836|    35|        71|106|\n",
						"|     16|2011-01-01|     1|  0|   1| 15|      0|      6|         0|         2|0.44|0.4394|0.77|   0.2985|    40|        70|110|\n",
						"|     17|2011-01-01|     1|  0|   1| 16|      0|      6|         0|         2|0.42|0.4242|0.82|   0.2985|    41|        52| 93|\n",
						"|     18|2011-01-01|     1|  0|   1| 17|      0|      6|         0|         2|0.44|0.4394|0.82|   0.2836|    15|        52| 67|\n",
						"|     19|2011-01-01|     1|  0|   1| 18|      0|      6|         0|         3|0.42|0.4242|0.88|   0.2537|     9|        26| 35|\n",
						"|     20|2011-01-01|     1|  0|   1| 19|      0|      6|         0|         3|0.42|0.4242|0.88|   0.2537|     6|        31| 37|\n",
						"+-------+----------+------+---+----+---+-------+-------+----------+----------+----+------+----+---------+------+----------+---+\n",
						"only showing top 20 rows\n",
						"\n",
						"+-------+------+------+---+----+---+-------+-------+----------+----------+----+-----+---+---------+------+----------+---+\n",
						"|instant|dteday|season| yr|mnth| hr|holiday|weekday|workingday|weathersit|temp|atemp|hum|windspeed|casual|registered|cnt|\n",
						"+-------+------+------+---+----+---+-------+-------+----------+----------+----+-----+---+---------+------+----------+---+\n",
						"|      0|     0|     0|  0|   0|  0|      0|      0|         0|         0|   0|    0|  0|        0|     0|         0|  0|\n",
						"+-------+------+------+---+----+---+-------+-------+----------+----------+----+-----+---+---------+------+----------+---+\n",
						"\n",
						"+-------+----------+------+---+----+---+-------+-------+----------+----------+----+------+----+---------+------+----------+-----+\n",
						"|instant|    dteday|season| yr|mnth| hr|holiday|weekday|workingday|weathersit|temp| atemp| hum|windspeed|casual|registered|  cnt|\n",
						"+-------+----------+------+---+----+---+-------+-------+----------+----------+----+------+----+---------+------+----------+-----+\n",
						"|      1|2011-01-01|     1|  0|   1|  0|      0|      6|         0|         1|0.24|0.2879|0.81|      0.0|   3.0|      13.0| 16.0|\n",
						"|      2|2011-01-01|     1|  0|   1|  1|      0|      6|         0|         1|0.22|0.2727| 0.8|      0.0|   8.0|      32.0| 40.0|\n",
						"|      3|2011-01-01|     1|  0|   1|  2|      0|      6|         0|         1|0.22|0.2727| 0.8|      0.0|   5.0|      27.0| 32.0|\n",
						"|      4|2011-01-01|     1|  0|   1|  3|      0|      6|         0|         1|0.24|0.2879|0.75|      0.0|   3.0|      10.0| 13.0|\n",
						"|      5|2011-01-01|     1|  0|   1|  4|      0|      6|         0|         1|0.24|0.2879|0.75|      0.0|   0.0|       1.0|  1.0|\n",
						"|      6|2011-01-01|     1|  0|   1|  5|      0|      6|         0|         2|0.24|0.2576|0.75|   0.0896|   0.0|       1.0|  1.0|\n",
						"|      7|2011-01-01|     1|  0|   1|  6|      0|      6|         0|         1|0.22|0.2727| 0.8|      0.0|   2.0|       0.0|  2.0|\n",
						"|      8|2011-01-01|     1|  0|   1|  7|      0|      6|         0|         1| 0.2|0.2576|0.86|      0.0|   1.0|       2.0|  3.0|\n",
						"|      9|2011-01-01|     1|  0|   1|  8|      0|      6|         0|         1|0.24|0.2879|0.75|      0.0|   1.0|       7.0|  8.0|\n",
						"|     10|2011-01-01|     1|  0|   1|  9|      0|      6|         0|         1|0.32|0.3485|0.76|      0.0|   8.0|       6.0| 14.0|\n",
						"|     11|2011-01-01|     1|  0|   1| 10|      0|      6|         0|         1|0.38|0.3939|0.76|   0.2537|  12.0|      24.0| 36.0|\n",
						"|     12|2011-01-01|     1|  0|   1| 11|      0|      6|         0|         1|0.36|0.3333|0.81|   0.2836|  26.0|      30.0| 56.0|\n",
						"|     13|2011-01-01|     1|  0|   1| 12|      0|      6|         0|         1|0.42|0.4242|0.77|   0.2836|  29.0|      55.0| 84.0|\n",
						"|     14|2011-01-01|     1|  0|   1| 13|      0|      6|         0|         2|0.46|0.4545|0.72|   0.2985|  47.0|      47.0| 94.0|\n",
						"|     15|2011-01-01|     1|  0|   1| 14|      0|      6|         0|         2|0.46|0.4545|0.72|   0.2836|  35.0|      71.0|106.0|\n",
						"|     16|2011-01-01|     1|  0|   1| 15|      0|      6|         0|         2|0.44|0.4394|0.77|   0.2985|  40.0|      70.0|110.0|\n",
						"|     17|2011-01-01|     1|  0|   1| 16|      0|      6|         0|         2|0.42|0.4242|0.82|   0.2985|  41.0|      52.0| 93.0|\n",
						"|     18|2011-01-01|     1|  0|   1| 17|      0|      6|         0|         2|0.44|0.4394|0.82|   0.2836|  15.0|      52.0| 67.0|\n",
						"|     19|2011-01-01|     1|  0|   1| 18|      0|      6|         0|         3|0.42|0.4242|0.88|   0.2537|   9.0|      26.0| 35.0|\n",
						"|     20|2011-01-01|     1|  0|   1| 19|      0|      6|         0|         3|0.42|0.4242|0.88|   0.2537|   6.0|      31.0| 37.0|\n",
						"+-------+----------+------+---+----+---+-------+-------+----------+----------+----+------+----+---------+------+----------+-----+\n",
						"only showing top 20 rows\n",
						"\n",
						"An error occurred: An error occurred while calling o370.save.\n",
						": java.lang.ClassNotFoundException: \n",
						"Failed to find data source: com.databricks.spark.redshift. Please find packages at\n",
						"https://spark.apache.org/third-party-projects.html\n",
						"       \n",
						"\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedToFindDataSourceError(QueryExecutionErrors.scala:574)\n",
						"\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:675)\n",
						"\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:725)\n",
						"\tat org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:864)\n",
						"\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:256)\n",
						"\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\n",
						"\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
						"\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
						"\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
						"\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
						"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
						"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
						"\tat py4j.Gateway.invoke(Gateway.java:282)\n",
						"\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
						"\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
						"\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
						"\tat java.lang.Thread.run(Thread.java:750)\n",
						"Caused by: java.lang.ClassNotFoundException: com.databricks.spark.redshift.DefaultSource\n",
						"\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\n",
						"\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n",
						"\tat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)\n",
						"\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n",
						"\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:661)\n",
						"\tat scala.util.Try$.apply(Try.scala:213)\n",
						"\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:661)\n",
						"\tat scala.util.Failure.orElse(Try.scala:224)\n",
						"\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:661)\n",
						"\t... 15 more\n",
						"\n",
						"Session closed successfully.\n",
						"/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:127: UserWarning: DataFrame constructor is internal. Do not directly use it.\n"
					]
				}
			],
			"source": [
				"# Create a new Glue job/session\n",
				"job = Job(glueContext)\n",
				"\n",
				"try:\n",
				"    # Step 1: Extract data from Glue Crawler\n",
				"    database_name = \"project2-patabase\"\n",
				"    table_name = \"project2_source_bucket\"\n",
				"\n",
				"    # Create a DynamicFrame from the Glue Catalog\n",
				"    dyf = glueContext.create_dynamic_frame.from_catalog(database=database_name, table_name=table_name)\n",
				"    dyf.printSchema()  # Print schema for reference\n",
				"\n",
				"    # Convert DynamicFrame to DataFrame\n",
				"    df = dyf.toDF()\n",
				"    df.show()\n",
				"\n",
				"    # Step 2: Count nulls in each column\n",
				"    null_counts = df.select([F.sum(F.col(c).isNull().cast(\"int\")).alias(c) for c in df.columns])\n",
				"    null_counts.show()\n",
				"\n",
				"    # Step 3: Define numeric columns\n",
				"    numeric_cols = ['temp', 'atemp', 'hum', 'windspeed', 'casual', 'registered', 'cnt']\n",
				"\n",
				"    # Function to replace outliers with the mean\n",
				"    def replace_outliers_with_mean(df, col_name):\n",
				"        # Calculate Q1, Q3, and IQR\n",
				"        quantiles = df.approxQuantile(col_name, [0.25, 0.75], 0.05)\n",
				"        Q1, Q3 = quantiles[0], quantiles[1]\n",
				"        IQR = Q3 - Q1\n",
				"        \n",
				"        # Define outlier bounds\n",
				"        lower_bound = Q1 - 1.5 * IQR\n",
				"        upper_bound = Q3 + 1.5 * IQR\n",
				"        \n",
				"        # Calculate the mean for the column\n",
				"        mean_value = df.select(F.mean(F.col(col_name))).collect()[0][0]\n",
				"        \n",
				"        # Replace outliers with the mean\n",
				"        df = df.withColumn(\n",
				"            col_name,\n",
				"            F.when((F.col(col_name) < lower_bound) | (F.col(col_name) > upper_bound), mean_value).otherwise(F.col(col_name))\n",
				"        )\n",
				"        \n",
				"        return df\n",
				"\n",
				"    # Step 4: Apply the function to each numeric column\n",
				"    for col_name in numeric_cols:\n",
				"        df = replace_outliers_with_mean(df, col_name)\n",
				"\n",
				"    # Show the updated DataFrame\n",
				"    df.show()\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 14,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"Error occurred while writing to Redshift: An error occurred while calling o83.getSink.\n",
						": java.sql.SQLException: The connection attempt failed.\n",
						"\tat com.amazon.redshift.util.RedshiftException.getSQLException(RedshiftException.java:56)\n",
						"\tat com.amazon.redshift.Driver.connect(Driver.java:319)\n",
						"\tat com.amazonaws.services.glue.util.JDBCWrapper$.$anonfun$connectionProperties$5(JDBCUtils.scala:1122)\n",
						"\tat com.amazonaws.services.glue.util.JDBCWrapper$.$anonfun$connectWithSSLAttempt$2(JDBCUtils.scala:1073)\n",
						"\tat scala.Option.getOrElse(Option.scala:189)\n",
						"\tat com.amazonaws.services.glue.util.JDBCWrapper$.$anonfun$connectWithSSLAttempt$1(JDBCUtils.scala:1073)\n",
						"\tat scala.Option.getOrElse(Option.scala:189)\n",
						"\tat com.amazonaws.services.glue.util.JDBCWrapper$.connectWithSSLAttempt(JDBCUtils.scala:1073)\n",
						"\tat com.amazonaws.services.glue.util.JDBCWrapper$.connectionProperties(JDBCUtils.scala:1118)\n",
						"\tat com.amazonaws.services.glue.util.JDBCWrapper.connectionProperties$lzycompute(JDBCUtils.scala:824)\n",
						"\tat com.amazonaws.services.glue.util.JDBCWrapper.connectionProperties(JDBCUtils.scala:824)\n",
						"\tat com.amazonaws.services.glue.util.JDBCWrapper.getRawConnection(JDBCUtils.scala:837)\n",
						"\tat com.amazonaws.services.glue.RedshiftDataSink.<init>(RedshiftDataSink.scala:39)\n",
						"\tat com.amazonaws.services.glue.GlueContext.getSink(GlueContext.scala:1142)\n",
						"\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
						"\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
						"\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
						"\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
						"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
						"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
						"\tat py4j.Gateway.invoke(Gateway.java:282)\n",
						"\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
						"\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
						"\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
						"\tat java.lang.Thread.run(Thread.java:750)\n",
						"Caused by: java.net.SocketTimeoutException: connect timed out\n",
						"\tat java.net.PlainSocketImpl.socketConnect(Native Method)\n",
						"\tat java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)\n",
						"\tat java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)\n",
						"\tat java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)\n",
						"\tat java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n",
						"\tat java.net.Socket.connect(Socket.java:607)\n",
						"\tat com.amazon.redshift.core.RedshiftStream.<init>(RedshiftStream.java:86)\n",
						"\tat com.amazon.redshift.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:111)\n",
						"\tat com.amazon.redshift.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:224)\n",
						"\tat com.amazon.redshift.core.ConnectionFactory.openConnection(ConnectionFactory.java:51)\n",
						"\tat com.amazon.redshift.jdbc.RedshiftConnectionImpl.<init>(RedshiftConnectionImpl.java:328)\n",
						"\tat com.amazon.redshift.Driver.makeConnection(Driver.java:474)\n",
						"\tat com.amazon.redshift.Driver.connect(Driver.java:295)\n",
						"\t... 23 more\n"
					]
				}
			],
			"source": [
				"\n",
				"# Step 5: Convert DataFrame to DynamicFrame\n",
				"dyf_processed = DynamicFrame.fromDF(df, glueContext, \"dyf_processed\")\n",
				"\n",
				"# Step 6: Define Redshift connection options\n",
				"redshift_options = {\n",
				"    \"url\": \"jdbc:redshift://project2-destination-cluster.cvkuuj2uqb2z.ap-south-1.redshift.amazonaws.com:5439/project2_redshift_database\",\n",
				"    \"user\": \"project2_user\",  # Replace with your Redshift username\n",
				"    \"password\": \"Shiv1234\",  # Replace with your Redshift password\n",
				"    \"dbtable\": \"processed_data\",  # The target table in Redshift\n",
				"    \"redshiftTmpDir\": \"s3://project2-redshift-tmp/\"  # Specify your temp S3 bucket\n",
				"}\n",
				"\n",
				"# Step 7: Write the DynamicFrame to Redshift\n",
				"try:\n",
				"    glueContext.write_dynamic_frame.from_options(\n",
				"        frame=dyf_processed,\n",
				"        connection_type=\"redshift\",\n",
				"        connection_options=redshift_options\n",
				"    )\n",
				"    print(\"Data written to Redshift successfully.\")\n",
				"except Exception as e:\n",
				"    print(f\"Error occurred while writing to Redshift: {str(e)}\")\n",
				"\n",
				"# Stop the Spark session (if needed)\n",
				"spark.stop() "
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": []
		}
	],
	"metadata": {
		"kernelspec": {
			"display_name": "Glue PySpark",
			"language": "python",
			"name": "glue_pyspark"
		},
		"language_info": {
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"file_extension": ".py",
			"mimetype": "text/x-python",
			"name": "Python_Glue_Session",
			"pygments_lexer": "python3"
		}
	},
	"nbformat": 4,
	"nbformat_minor": 4
}
